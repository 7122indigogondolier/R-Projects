---
title: "Project 2 - Geospatial Mapping and Webscraping"
author: "Utkrist P. Thapa"
date: "`r Sys.Date()`"
output: html_document
---

### Load Packages 
I have borrowed a 'multiplot' function from an outside source. Multiplot was supposed to be part of ggplot2 package but I ran into errors with that so I simply copied and pasted this function here. I have included this source in the Project Log.
```{r load-packages, message = FALSE}
library(tidyverse)
library(kableExtra)
library(rvest)
library(robotstxt)
library(ggmap)
library(maps)
library(ggplot2)
library(knitr)

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

### Citing ggmap
D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal,
5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf

### Check Permission to Scrape 
Checking permission to scrape from ratebeer.com.
```{r check-perm, message = FALSE, warning = FALSE}
paths_allowed("https://www.ratebeer.com/")
```

### Exercise 1
I have written a function named get_links that takes the state's url as its argument and returns a vector of links for meaderies, cideries, and sake producers.
```{r ex1}
get_links <- function(state_url) {
  
  state_page <- read_html(state_url)
  
  state_page %>%
  html_nodes(".inactive a") %>%
  html_attr("href") %>%
  paste("https://www.ratebeer.com", ., sep = "")
}

nc_url <- "https://www.ratebeer.com/breweries/unitedstates/33/213/"
nc_links <- get_links(nc_url)
```

### Exercise 2
I have written a function named get_info that takes in the producer url (meadery, cidery or sake prodcuer) as its argument and returns a dataframe with all those producers information scraped from this url.

I have also made a make_pretty function to reduce repition with kable styling.
```{r ex2, message = FALSE, warning = FALSE}
get_info <- function(producer_url) {
  producer_page <- read_html(producer_url)

  producers <- producer_page %>%
    html_nodes("#brewerTable a:nth-child(1)") %>%
    html_text() %>%
    str_trim()

  cities <- producer_page %>%
    html_nodes(".filter") %>%
    html_text()
  
  if (length(cities) != length(producers)) {
    add_cities <- producer_page %>%
      html_nodes("#brewerTable span") %>%
      html_text()
    cities <- c(cities, add_cities)
  } 

  types <- producer_page %>%
    html_nodes("td.hidden-sm") %>%
    html_text()

  ests <- producer_page %>%
    html_nodes("td:nth-child(5)") %>%
    html_text() %>%
    str_trim() %>%
    as.numeric()

  counts <- producer_page %>%
    html_nodes(".hidden-sm+ td") %>%
    html_text() %>%
    str_trim() %>%
    as.numeric()

  active_num <- producer_page %>%
    html_nodes("br+ .nav-tabs .active a") %>%
    html_text() %>%
    substr(1,2) %>%
    as.numeric()

  active_num <- ifelse(is.na(active_num) || length(active_num) == 0, 0, active_num)
  active <- rep("Active", active_num)
  
  closed_num1 <- producer_page %>%
    html_nodes(".active+ li a") %>%
    html_text() %>%
    substr(1,2) %>%
    as.numeric()
  
  closed_num2 <- producer_page %>%
    html_nodes("br+ .nav-tabs .active+ li a") %>%
    html_text() %>%
    substr(1,2) %>%
    as.numeric()
  
  closed_num1 <- ifelse(is.na(closed_num1) || length(closed_num1) == 0, 0, closed_num1)
  closed_num2 <- ifelse(is.na(closed_num2) || length(closed_num2) == 0, 0, closed_num2)
  
  if (closed_num1 > 0 && closed_num2 > 0) {
    closed_num <- closed_num1
  }
  else {
    closed_num <- closed_num1 + closed_num2
  }
  
  closed <- rep("Closed", closed_num)

  status_all <- c(active, closed)

  tibble(
    producer = producers,
    city = cities,
    type = types,
    est = ests,
    count = counts,
    status = status_all
  )
}

make_pretty <- function(x, n) {
  x %>%
    head(n) %>% 
    kable() %>%
    kable_styling()
}

get_info(nc_links[1]) %>% make_pretty(10)
get_info(nc_links[2]) %>% make_pretty(10)
get_info(nc_links[3]) %>% make_pretty(10)
```
### Exercise 3
I simply use the get_info function to get information from meaderies, cideries and sake producers in Wyoming.
```{r ex3, message = FALSE, warning = FALSE}
wy_url <- "https://www.ratebeer.com/breweries/unitedstates/51/213/"
wy_links <- get_links(wy_url)

get_info(wy_links[1]) %>% make_pretty(10)
get_info(wy_links[2]) %>% make_pretty(10)
get_info(wy_links[3]) %>% make_pretty(10)
```

### Exercise 4
I have used the two functions defined in earlier exercises in a new function that takes in the url for the states page as its only input argument. Then, I use the function get_links() to get the links to the three categories: meaderies, cideries and sake producers. I run a for loop on the links vector to access the three categories. I use the function get_info() inside the for loop to get the information for each category of producer. Finally, I combined the dataframe for each category using the base R function rbind().
```{r ex4, message = FALSE, warning = FALSE}
get_combined_df <- function(state_url) {
  links <- get_links(state_url)
  for (i in 1:3) {
    if (i == 1) {
      combined_df <- get_info(links[i])
    } else {
      combined_df <- rbind(combined_df, get_info(links[i]))
    }
  }
  combined_df
}
```

#### North Carolina
```{r nc-combined, message = FALSE, warning = FALSE}
nc_df <- get_combined_df(nc_url) 
nc_df %>% make_pretty(15)
```

#### California
```{r ca-combined, message = FALSE, warning = FALSE}
ca_url <- "https://www.ratebeer.com/breweries/unitedstates/5/213/"
ca_df <- get_combined_df(ca_url) 
ca_df %>% make_pretty(15)
```

#### Massachussetts 
```{r ma-combined, message = FALSE, warning = FALSE}
ma_url <- "https://www.ratebeer.com/breweries/unitedstates/21/213/"
get_combined_df(ma_url) %>% make_pretty(15)
```

#### Virginia
```{r va-combined, message = FALSE, warning = FALSE}
va_url <- "https://www.ratebeer.com/breweries/unitedstates/46/213/"
get_combined_df(va_url) %>% make_pretty(15)
```

#### Wyoming
```{r wy-combined, message = FALSE, warning = FALSE}
get_combined_df(wy_url) %>% make_pretty(15)
```

### Exercise 5

#### Mapping Setup 
I register for Google's API here.
```{r mapping_setup}

```

I have commented out the code and set eval = FALSE as instructed. Here, I use mutate geocode on distinct cities, assign it to a dataframe, and store it in a csv file. I also join the information into the original dataframe and store all that information into a csv file as well.
```{r ex5, eval = FALSE}
# nc_lon_lat <- nc_df %>%
#   distinct(city) %>%
#   mutate(city_state = paste(city, "North Carolina", sep = ", ")) %>%
#   mutate_geocode(city_state)

# nc_df_joined <- nc_df %>%
#   inner_join(nc_lon_lat) 

# ca_lon_lat <- ca_df %>%
#   distinct(city) %>%
#   mutate(city_state = paste(city, "California", sep = ", ")) %>%
#   mutate_geocode(city_state)

# ca_df_joined <- ca_df %>%
#   inner_join(ca_lon_lat)

# write_csv(nc_df_joined, file = "data/nc_data.csv")
# write_csv(ca_df_joined, file = "data/ca_data.csv")
# write_csv(nc_lon_lat, file = "data/nc_lon_lat.csv")
# write_csv(ca_lon_lat, file = "data/ca_lon_lat.csv")
```

### Load Data
I load the csv data created earlier into dataframes here.
```{r load-data}
nc_data <- read_csv("data/nc_data.csv")
ca_data <- read_csv("data/ca_data.csv")
nc_lon_lat <- read_csv("data/nc_lon_lat.csv")
ca_lon_lat <- read_csv("data/ca_lon_lat.csv")
```

### Exercise 6
I count the number of producers by city and type here and plot them onto the maps obtained from google. I have set the scale for the size of the dots to be uniform for both plots using scale_size_continuous.
```{r ex6}
nc_total_brewers <- nc_data %>%
  group_by(city, type) %>%
  summarise(total_brewers = n()) %>%
  ungroup()

nc_total_brewers_joined <- nc_total_brewers %>%
  inner_join(nc_lon_lat)

ca_total_brewers <- ca_data %>%
  group_by(city, type) %>%
  summarise(total_brewers = n()) %>%
  ungroup()

ca_total_brewers_joined <- ca_total_brewers %>%
  inner_join(ca_lon_lat)

nc <- get_map(location = "north carolina", maptype = "terrain", source = "google", zoom = 6) 
p1 <- ggmap(nc, size = c(600, 600)) +
  geom_point(data = nc_total_brewers_joined, 
             aes(x = lon, y = lat, size = total_brewers, color = type, alpha = 0.25)) +
  labs(title = "Producers in NC", 
       size = "Total Producers", 
       color = "Type")  +
  scale_size_continuous(limits = c(1, 7)) +
  theme(legend.position = "bottom", 
        legend.key.size = unit(0.3, "cm"), 
        legend.box = "vertical", 
        legend.title = element_text(size = 8))

ca <- get_map(location = "california", maptype = "terrain", source = "google", zoom = 6) 
p2 <- ggmap(ca, size = c(600, 600)) +
  geom_point(data = ca_total_brewers_joined, 
             aes(x = lon, y = lat, size = total_brewers, color = type, alpha = 0.25)) +
  labs(title = "Producers in CA",
       size = "Total Producers",
       color = "Type") +
  scale_size_continuous(limits = c(1, 7)) +
  theme(legend.position = "bottom", 
        legend.key.size = unit(0.3, "cm"), 
        legend.box = "vertical", 
        legend.title = element_text(size = 8))

multiplot(p1, p2, cols = 2)
```

### Exercise 7
I filter the producers located in each metropolitan area using lat and lon value range for the cities. Then I use the same code as in Exercise 6, simply replaced with the relevant information from the filtered dataframe.
```{r ex7}
la_data <- ca_total_brewers_joined %>%
  filter(lon <= -118,
         lon > -118.6, 
         lat > 33.8, 
         lat < 34.3)

sf_data <- ca_total_brewers_joined %>%
  filter(lon <= -122.2,
         lon > -122.7, 
         lat > 37.5, 
         lat < 38)

la <- get_map(location = "los angeles", maptype = "terrain", source = "google", zoom = 11) 
p1 <- ggmap(la, size = c(600, 600)) +
  geom_point(data = la_data, 
             aes(x = lon, y = lat, size = total_brewers, color = type, alpha = 0.25)) +
  labs(title = "Producers in LA", 
       size = "Total Producers", 
       color = "Type")  +
  scale_size_continuous(limits = c(1, 7)) +
  theme(legend.position = "bottom", 
        legend.key.size = unit(0.3, "cm"), 
        legend.box = "vertical", 
        legend.title = element_text(size = 8))

sf <- get_map(location = "san francisco", maptype = "terrain", source = "google", zoom = 11) 
p2 <- ggmap(sf, size = c(600, 600)) +
  geom_point(data = sf_data, 
             aes(x = lon, y = lat, size = total_brewers, color = type, alpha = 0.25)) +
  labs(title = "Producers in SF",
       size = "Total Producers",
       color = "Type") +
  scale_size_continuous(limits = c(1, 7)) +
  theme(legend.position = "bottom", 
        legend.key.size = unit(0.3, "cm"), 
        legend.box = "vertical", 
        legend.title = element_text(size = 8))

multiplot(p1, p2, cols = 2)
```


### Project Log 
Side by side maps: https://www.r-spatial.org/r/2018/10/25/ggplot2-sf-3.html 
Multiplot function: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
scale_size_continuous: https://stackoverflow.com/questions/16143811/size-of-points-in-ggplot2-comparable-across-plots
How to change legend size: https://www.statology.org/ggplot2-legend-size/ 

### The Pledge 
On my honor, I have neither given nor received any unacknowledged aid on this test.

Utkrist P. Thapa 

April 7, 2021, Wednesday